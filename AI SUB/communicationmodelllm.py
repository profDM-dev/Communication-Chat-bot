# -*- coding: utf-8 -*-
"""CommunicationMOdelLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X81FtFIPXduq3ZKVByPWXb9cE2-OiS4p
"""

!pip install -q pandas transformers

import pandas as pd
from transformers import pipeline

# ⿡ Load CSV
df = pd.read_excel("/content/drive/MyDrive/CommunicationChat/communication_chatbot_dataset.xlsx").fillna("")

# ⿢ Combine all data into one text
context = df.astype(str).agg('. '.join, axis=1)
context_text = "\n".join(context)

# ⿣ Load local pretrained model
generator = pipeline("text2text-generation", model="google/flan-t5-base")

# ⿤ Ask function
def ask(query):
    prompt = f"Answer the question. If not available, say 'No information found.'\n\nData:\n{context_text}\n\nQuestion: {query}\nAnswer:"
    answer = generator(prompt, max_new_tokens=150, do_sample=False)[0]['generated_text']
    print("\n--- ANSWER ---\n",answer.strip())

ask("tell me about we was late")